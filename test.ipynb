{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2225b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    CamembertTokenizer, \n",
    "    CamembertForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82da34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup instructions:\n",
    "# conda create -n city_extraction python=3.9\n",
    "# conda activate city_extraction\n",
    "# conda install pytorch torchvision torchaudio -c pytorch\n",
    "# pip install transformers pandas scikit-learn datasets\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('fake_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29f06b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data for token classification\n",
    "class CityExtractionDataset(Dataset):\n",
    "    def __init__(self, sentences, origins, destinations, tokenizer, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.origins = origins\n",
    "        self.destinations = destinations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences[idx])\n",
    "        origin = str(self.origins[idx]) if pd.notna(self.origins[idx]) else \"\"\n",
    "        destination = str(self.destinations[idx]) if pd.notna(self.destinations[idx]) else \"\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Create labels: 0=O, 1=B-ORIGIN, 2=I-ORIGIN, 3=B-DEST, 4=I-DEST\n",
    "        labels = [0] * self.max_length\n",
    "        offset_mapping = encoding.pop('offset_mapping')\n",
    "        \n",
    "        # Tag origin city\n",
    "        if origin and origin in sentence:\n",
    "            origin_start = sentence.lower().find(origin.lower())\n",
    "            if origin_start != -1:\n",
    "                origin_end = origin_start + len(origin)\n",
    "                for i, (start, end) in enumerate(offset_mapping):\n",
    "                    if start >= origin_start and end <= origin_end and start != end:\n",
    "                        labels[i] = 1 if start == origin_start or i == 0 or labels[i-1] == 0 else 2\n",
    "        \n",
    "        # Tag destination city\n",
    "        if destination and destination in sentence:\n",
    "            dest_start = sentence.lower().find(destination.lower())\n",
    "            if dest_start != -1:\n",
    "                dest_end = dest_start + len(destination)\n",
    "                for i, (start, end) in enumerate(offset_mapping):\n",
    "                    if start >= dest_start and end <= dest_end and start != end:\n",
    "                        if labels[i] == 0:  # Don't override origin labels\n",
    "                            labels[i] = 3 if start == dest_start or i == 0 or labels[i-1] not in [3, 4] else 4\n",
    "        \n",
    "        encoding['labels'] = labels\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c26aa808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model (use Fast tokenizer)\n",
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained('camembert-base')\n",
    "model = CamembertForTokenClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=5  # O, B-ORIGIN, I-ORIGIN, B-DEST, I-DEST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8519559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1ed7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = CityExtractionDataset(\n",
    "    train_df['sentence'].values,\n",
    "    train_df['ville_origine'].values,\n",
    "    train_df['ville_arrivee'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = CityExtractionDataset(\n",
    "    val_df['sentence'].values,\n",
    "    val_df['ville_origine'].values,\n",
    "    val_df['ville_arrivee'].values,\n",
    "    tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1edbbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea385a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e75e4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99dbd988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 05:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.011521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.003584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.0714250594060868, metrics={'train_runtime': 319.449, 'train_samples_per_second': 150.259, 'train_steps_per_second': 9.391, 'total_flos': 3135646126080000.0, 'train_loss': 0.0714250594060868, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe01e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./city_extraction_model/tokenizer_config.json',\n",
       " './city_extraction_model/special_tokens_map.json',\n",
       " './city_extraction_model/sentencepiece.bpe.model',\n",
       " './city_extraction_model/added_tokens.json',\n",
       " './city_extraction_model/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save model\n",
    "model.save_pretrained('./city_extraction_model')\n",
    "tokenizer.save_pretrained('./city_extraction_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b7eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def extract_cities(sentence, model, tokenizer):\n",
    "    # Get the device the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move all inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    origin_tokens = []\n",
    "    dest_tokens = []\n",
    "    \n",
    "    for token, pred in zip(tokens, predictions[0]):\n",
    "        if pred == 1 or pred == 2:  # ORIGIN\n",
    "            if token not in ['<s>', '</s>', '<pad>']:\n",
    "                origin_tokens.append(token.replace('▁', ''))\n",
    "        elif pred == 3 or pred == 4:  # DEST\n",
    "            if token not in ['<s>', '</s>', '<pad>']:\n",
    "                dest_tokens.append(token.replace('▁', ''))\n",
    "    \n",
    "    origin = ''.join(origin_tokens).strip()\n",
    "    destination = ''.join(dest_tokens).strip()\n",
    "    \n",
    "    return origin, destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7005999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origine: Lyon\n",
      "Destination: Paris\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "test_sentence = \"Je veux aller à Paris en partant de Lyon demain\"\n",
    "origin, destination = extract_cities(test_sentence, model, tokenizer)\n",
    "print(f\"Origine: {origin}\")\n",
    "print(f\"Destination: {destination}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
